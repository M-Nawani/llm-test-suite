# ğŸ§ª LLM Evaluation Test Suite

This repository contains a **Pytest-based testing framework** for evaluating **Large Language Models (LLMs)** running on [Ollama](https://ollama.ai/).  
It focuses on **robustness**, **performance**, **context handling**, and **hallucination detection**.

## ğŸ“‚ Project structure

```plaintext
llm-test-suite/
â”œâ”€â”€ llmtestclient.py                              # Ollama LLM test client (config + helpers)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_basic_functionality.py                # Basic behavior & sanity checks
â”‚   â”œâ”€â”€ test_context_learning.py                   # Context retention & in-context learning
â”‚   â”œâ”€â”€ test_context_processing_generation.py      # Structured output & JSON validation
â”‚   â”œâ”€â”€ test_performance.py                        # Throughput & latency benchmarks
â”‚   â”œâ”€â”€ test_robustness.py                         # Prompt injection & logical consistency
â”‚   â”œâ”€â”€ test_hallucination.py                      # Hallucination detection & factual tests
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ README.md
```

## ğŸš€ Features

- **Model Availability Check** â€“ Skips tests if the LLM is unavailable.
- **Context Learning Tests** â€“ Checks if the model retains information across prompts.
- **Structured Output Tests** â€“ Validates JSON and list generation.
- **Performance Metrics** â€“ Measures latency, token throughput.
- **Robustness Checks** â€“ Tests against prompt injection, nonsensical queries, and logical consistency.
- **Hallucination Detection** â€“ Ensures factual accuracy in closed domains.


## ğŸ“¦ Installation

```bash
# Clone the repo
git clone https://github.com/m-nawani/llm-test-suite.git
cd llm-test-suite
```

# Install dependencies
pip install -r requirements.txt

## âš™ï¸ Configuration

You can configure the **OllamaTestClient** in `llmtestclient.py` to change the model, host, or port.

| Setting   | Default Value | Description |
|-----------|--------------|-------------|
| `model`   | `tinyllama`  | The Ollama model to use (e.g., `mistral`, `llama2-7b`) |
| `host`    | `localhost`  | The API server hostname |
| `port`    | `11434`      | The API server port |

Example:
```python
from llmtestclient import OllamaTestClient

# Use a different model and server
client = OllamaTestClient(model="mistral", host="localhost", port=11434)
```

## ğŸ§ª Running All Tests

- To run the full LLM test suite:

```bash
pytest -v
```

- To run only specific test files:

```bash
pytest tests/test_performance.py
pytest tests/test_robustness.py
```

- To run a single test function

```bash
pytest tests/test_robustness.py::test_prompt_injection_resilience -q
```

## ğŸ“Œ Notes

- Designed for lightweight Ollama models (`tinyllama`, `llama2-7b`, `mistral`) but adaptable for remote endpoints.  
- Mock or sandbox external factual sources to keep hallucination tests deterministic.  
- Mark heavy benchmarks as **slow** to keep the default test run fast.  
- Extend the suite with domain-specific tests and schema validation for structured outputs.

## ğŸš§ Work in Progress

- Containerize the application for consistent local and CI environments.  
- Add CI/CD pipeline to automate:
  - Build
  - Test
  - Deployment
- Integrate with existing monitoring and logging setup.


## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).





